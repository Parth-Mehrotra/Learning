\documentclass{article}
\usepackage{amsmath}

\begin{document}
	\title{Discrete II Notes}
	\author{Parth Mehrotra}
	\maketitle

	\setlength{\parindent}{0cm} {
		\section*{\Large{\textbf{Set Theory Review}}}

			Sets are collections of unique elements.

			\subsection*{Complement}
				Everything that's not in the set. Denoted by \(A'\),  \(A^C\), or \(\overline{A}\)
			\subsection*{Intersection}
				An operation that takes two sets, and returns the common elements between them. Denoted by \(A \cap B\)

			\subsection*{Union}
				An operation that takes two sets, and returns a all the elements that are in \(A\), \(B\), and \(A \cap B\). Denoted by \( A \cup B \).

			\subsection*{De Morgan's Laws}
				\[ (A \cup B)^C = A^C \cap B^C \]
				\[ (A \cap B)^C = A^C \cup B^C \]

			\subsection*{Disjoint Sets}
				\textbf{Disjoint} sets or \textbf{Mutually Exclusive} sets, are sets that have no elements in common. More formally: \((A \cup B) = \emptyset\)

		\section*{\Large{\textbf{Intro to Probability}}}
			\textbf{Sample Space} of an experiment is the set of all possible outcomes of that experiment.
			An \textbf{event} is any collection (subset) of outcomes contained in the sample space S. An event is said to be \textbf{simple} if it consists of exactly one outcome and \textbf{compound} if it consists of more than one outcome.

		\section*{\Large{\textbf{Counting}}}
			For an ordered pair defined by \((x, y)\) where \(x\) can be selected in \(n_1\) ways, and \(y\) can be selected in \(n_2\) ways, the number of pairs is \(n_1 n_2\). Can be extended to \(k\) dimensions. This is known as the \textbf{Multiplication rule}.

			\subsection*{Permutations}
				For \(k\) selections made \textbf{with replacement} on \(n\) distinct elements, there are \(n^k\) possible outcomes.

				\textbf{Without replacement} however, there are \(n\) options for the first selection, \(n-1\) choices for the next selection, and \(n-k+1\) choice(s) for the \(k^{th}\) selection. This yeilds.

				\[_{n}P_{k} = n(n-1)(n-2) \ldots (n-k+1)\]

			\subsection*{Combinations}
				Given \(n\) distinct objects, the number of \textbf{unordered} subsets of size \(k\) is given by \(_{n}C_{k}\), or \(\binom{n}{k}\) (\(n\) choose \(k\)).

				\[_{n}C_{k} = \frac{n!}{(n-k)!(k!)} \]

			\subsection*{Overcounting with Groups}
				For \(n\) distinct objects being devided into \(k\) groups, \(\binom{n}{k} \) over counts by a factor of \(k!\). To account for that, we do the following:
				\[
					\frac{\binom{n}{k}}{k!}
				\]

			\subsection*{Bose Einstein}
				For counting ways to separate \(n\) indistinguisable objects into \(k\) groups. We can use the following:
					\[
						\binom{n - 1 + k}{k}
					\]

				Analogy: For splitting \(n\) peices of candies between \(k\) kids, add \(k-1\) placeholder candies. \(\binom{n-1+k}{k}\) represents all the ways the dividers can be placed, such that the kids get the candy.

			\subsection*{Counting methods summary}
				\begin{center}
					\begin{tabular}{ |c| c c| }
					\hline
					& With order & Without Order \\
					\hline
					With replacement & \( n^k \) & \( \binom{n-1+k}{k}\) \\
					\hline
					Without Replacement & \(\frac{n!}{(n-k)!}\) & \(\binom{n}{k} \) \\
					\hline
					\end{tabular}
				\end{center}

		\section*{\Large{\textbf{Set inclusion/exclusion principal}}}

			\[ |A \cup B| = |A| + |B| - |A \cap B| \]
			\[ |A \cup B \cup C| = |A| + |B| + |C| - |A \cap B| - |A \cap B| - |B \cap C| + |A \cap B \cap C| \]
		\section*{\Large{\textbf{Conditional Probability}}}
			\(P(A | B)\) represents the probability that \(A\) happens, given that \(B\) \emph{already} happened.
			\[
				P(A | B) = \frac{P(A \cap B)}{P(B)}
			\]

			\subsection*{Bayes' Theorem}
				
				Sometimes we don't have some of those probabilities. In this situation, we can use Bayes' Theorem:
				\[
					P(A|B) = \frac{P(B | A) P(A)}{P(B)} 
				\]

			\subsection*{Law of total probability}
				For an \(A, B\) where \( P(B) \neq 0 \) and \( P(B) \neq 1\)

				\[
					P(A) = P(A|B) \cdot P(B) + P(A | B^C) \cdot P(B^C)
				\]

			\subsection*{Multiplication Rule for \(P(A \cap B)\)} 
				\[
					P(A \cap B) = P(A|B) \cdot P(B) = P(B|A) \cdot P(A)
				\]

			\subsection*{Independence}
				Two Events A and B are independent if \(P(A|B) = P(A)\), and are dependent otherwise. 

				A and B are independent if and only if:
				\[
					P(A \cap B) = P(A) \cdot P(B)
				\]

			\subsection*{Multiple levels of independence}
				Types of independence between n events:
				\begin{itemize}
					\item Pairwise independence all pairs of events within n are independent.
					\item K-wise independence all k sets of events are independent.
					\item Mutual independence all k-wise sets are independent.
				\end{itemize} 

		\section*{\Large{\textbf{Random Variables}}}
			
			For a given sample space, a \textbf{random variable} is any rule that associates a number with each outcome. Customarily denoted by uppercase letters, such as X and Y, near the end of our alphabet. Lowercase letters are used to represent some particular value of the corresponding random variable.  \\

			Any random variable whose only possible values are 0 and 1 is called a \textbf{Bernoulli random variable}. (1 usually indicates success) \\ 

			A \textbf{discrete} random variable's values are either a finite set of integers. They are countable. \\ 

			A \textbf{continuous} random variable can take on any value within a given interval.

			\subsection*{Probability distribution}
				A \textbf{probability distribution} or a \textbf{probability mass function} of a discrete random variable is defined for every number \(x\). For every possible value x of the random variable, the pmf specifies the probability of observing that value when the experiment is performed.

			\subsection*{Binomial distribution}
				An experiment where there are 2 outcomes with a given probability, with n trials.
				Criteria for a binomial distribution:

				\begin{enumerate}
					\item n trials
					\item 2 possible outcomes per trial
					\item trials are independent
					\item a single p that does not change
				\end{enumerate}

			\subsection*{Cumulative Distribution Function}
				For a probability distribution at a fixed point \(x\), we wish to compute the probability that the observed value of \(X\) will be at most \(x\). We're interested in the sum of all the values until, this point.

				The \textbf{CDF} is defined by:

				\[
					F(X) = P(X \leq x) = \sum\limits_{y:y \leq x} p(y)
				\]

\end {document}
